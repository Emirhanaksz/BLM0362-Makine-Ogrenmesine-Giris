# ğŸ” HAKKINDA
Bu proje; Bursa Teknik Ãœniversitesi 2022 GÃ¼z DÃ¶nemi dersi olan Bilgisayar MÃ¼hendisliÄŸine GiriÅŸ dersi iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r. 

ğŸ‘¨ğŸ¾â€ğŸ’» Sunum videosu âœ [`tÄ±klayÄ±n`](https://youtu.be/-LmVy9_3oUI)


# **Perceptron, Yapay sinir aÄŸlarÄ±, RNN algoritmasÄ±**

*Emirhan SelÃ§uk AksÃ¶z*

## Ä°Ã§reik Tablosu

1. [Perceptron](#Perceptron)
2. [Yapay Sinir AÄŸlarÄ±](#YSA)
3. [RNN AlgoritmasÄ±](#RNN)

<br>

# **1.	Perceptron**


Perceptron (AlgÄ±layÄ±cÄ±), tek katmanlÄ± bir yapay sinir aÄŸÄ±nÄ±n temel birimidir. EÄŸitilebilecek tek bir yapay sinir hÃ¼cresinden oluÅŸmaktadÄ±r. Denetimli bir Ã¶ÄŸrenme algoritmasÄ±dÄ±r. Ä°lk kez 1957 yÄ±lÄ±nda Cornell Ãœniversitesiâ€™nden psikolog Frank Rosenbatt tarafÄ±ndan ortaya atÄ±lmÄ±ÅŸtÄ±r.

![GÃ¶rsel 1](./doc_img/1.png "GÃ¶rsel 1")

Bir perceptron dÃ¶rt bÃ¶lÃ¼mden oluÅŸmaktadÄ±r. Bunlar :

*GiriÅŸ deÄŸerleri
*AÄŸÄ±rlÄ±klar ve sapma
*AÄŸÄ±rlÄ±klÄ± toplam
*Aktivasyon iÅŸlevi



# 1.1.	Perceptron TanÄ±m

Matematiksel olarak, bir algÄ±layÄ±cÄ±yÄ±, aÄŸÄ±rlÄ±klarÄ±n, giriÅŸlerin ve sapmalarÄ±n (dikey sapma) bir fonksiyonu olarak gÃ¶sterebiliriz: f(x)=w.x+b

![GÃ¶rsel 1](./doc_img/2.png "GÃ¶rsel 1")

<br>
<br>

![GÃ¶rsel 1](./doc_img/3.png "GÃ¶rsel 1")

y: Girdiye ait skoru verir.
x: Girdi(input)
W: AÄŸÄ±rlÄ±k parametresi(weight)
b: Bias deÄŸeri
Yapay sinir aÄŸlarÄ± ya da derin Ã¶ÄŸrenme modelinde yapÄ±lan temel iÅŸlem; modelin en iyi skoru(y) vereceÄŸi 


# 1.2.	Perceptron Ã–ÄŸrenme AlgoritmasÄ±

![GÃ¶rsel 1](./doc_img/4.jpg "GÃ¶rsel 1")

<br>

YukarÄ±daki ÅŸekilde tanÄ±mlanan aÄŸÄ±n yapay sinir aÄŸÄ±ndaki karÅŸÄ±lÄ±ÄŸÄ± perceptronâ€™dur. Bu fonksiyonda yukarÄ±da gÃ¶sterildiÄŸi Ã¼zere W deÄŸeri aÄŸÄ±rlÄ±k parametresi, x deÄŸeri girdi, b deÄŸeri bias ve y deÄŸeride aÄŸÄ±n Ã§Ä±ktÄ±sÄ± olarak tanÄ±mlanmaktadÄ±r. Burada x girdi deÄŸerimiz, Ã¶rneÄŸin kedi resimlerini tanÄ±yorsak kedi resmine ait matrisi, y ise bu resmin kediye ne kadar benzediÄŸine dair skoru verir. Parametrelerimiz olan W aÄŸÄ±rlÄ±k ve b bias deÄŸerlerini bu Ã§Ä±ktÄ± skorunu iyileÅŸtirmek iÃ§in kullanÄ±lÄ±r.
<br>


# 1.3.	Perceptron KullanÄ±m AlanlarÄ±
Perceptron genellikle verilerin iki bÃ¶lÃ¼me ayrÄ±lmasÄ±na olanak saÄŸlar bu nedenle DoÄŸrusal Ä°kili SÄ±nÄ±flandÄ±rÄ±cÄ± olarak da adlandÄ±rÄ±lmaktadÄ±r. Perceptron Ã¶ÄŸrenme algoritmasÄ±nÄ±n amacÄ±, pozitif girdileri ve negatif girdileri doÄŸru sÄ±nÄ±flandÄ±rabilen bir karar sÄ±nÄ±rÄ± (Ã§izgi) oluÅŸturmaktÄ±r. DoÄŸru sÄ±nÄ±r deÄŸerine ulaÅŸÄ±lmasÄ± iÃ§in girdi ve Ã§Ä±ktÄ± verilerinin fazla olmasÄ± gerekmektedir. Model lineer olarak ayrÄ±labilirse perceptron algoritmasÄ±nÄ±n kesin sonuÃ§ Ã¼retmesi beklenir. Ancak sistem lineer olarak ayrÄ±lamÄ±yorsa perceptron algoritmasÄ± kÃ¶tÃ¼ sonuÃ§ Ã¼retecektir ve modeli sÄ±nÄ±flandÄ±ramayacaktÄ±r.
<br>

![GÃ¶rsel 1](./doc_img/5.png "GÃ¶rsel 1")

<br>

# 1.4.	Perceptron SÄ±nÄ±rlamalarÄ±

Perceptron, doÄŸrusal olmayan ayrÄ±labilir veri noktalarÄ±nÄ± sÄ±nÄ±flandÄ±ramaz. Ã‡ok katmanlÄ± parametreleri iÃ§eren karmaÅŸÄ±k problemler, Perceptronla Ã§Ã¶zÃ¼lemez. Perceptron, lineer olmayan ayrÄ±labilir veri noktalarÄ±nÄ± sÄ±nÄ±flandÄ±ramaz. YukarÄ±da karÅŸÄ±laÅŸÄ±lan problemlere Ã§Ã¶zÃ¼m iÃ§in farklÄ± yollarla baÄŸlanan ve farklÄ± aktivasyon fonksiyonlarÄ±nda Ã§alÄ±ÅŸan perceptronlarÄ±n bir bileÅŸimi olan MultiLayer Perceptron kullanÄ±lmaktadÄ±r.
<br>

![GÃ¶rsel 1](./doc_img/6.png "GÃ¶rsel 1")



<br>

# **2.	Yapay Sinir AÄŸlarÄ±**

Yapay sinir aÄŸlarÄ± (YSA), insan beyninin bilgi iÅŸleme tekniÄŸinden esinlenerek geliÅŸtirilmiÅŸ bir bilgi iÅŸlem teknolojisidir.YSA, insan beyninin Ã¶ÄŸrenme yolunu taklit ederek beynin Ã¶ÄŸrenme, hatÄ±rlama, genelleme yapma yolu ile topladÄ±ÄŸÄ± verilerden yeni veri Ã¼retebilme gibi temel iÅŸlevlerin gerÃ§ekleÅŸtirildiÄŸi bilgisayar yazÄ±lÄ±mlarÄ±dÄ±r. Yapay sinir aÄŸlarÄ±; insan beyninden esinlenerek, Ã¶ÄŸrenme sÃ¼recinin matematiksel olarak modellenmesi uÄŸraÅŸÄ± sonucu ortaya Ã§Ä±kmÄ±ÅŸtÄ±r.Yapay sinir aÄŸlarÄ±, paralel daÄŸÄ±tÄ±lmÄ±ÅŸ aÄŸlar, baÄŸlantÄ±lÄ± aÄŸlar, nuromorfik aÄŸlar gibi adlarla da tanÄ±mlanmaktadÄ±r.

<br>

# 2.1.	YSAâ€™nÄ±n Genel Ã–zellikleri
Yapay sinir aÄŸlarÄ± canlÄ±lardaki sinir sistemlerinin aÅŸaÄŸÄ±daki iÅŸlevlerini gerÃ§ekleÅŸtirmeyi hedefler:

*Ã–ÄŸrenme
*Ä°liÅŸkilendirme
*SÄ±nÄ±flandÄ±rma
*Genelleme
*Tahmin
*Ã–zellik Belirleme 


<br>

# 2.2.	YSAâ€™nÄ±n AvantajlarÄ±

DoÄŸrusal Olmama, Paralellik, Hata ToleransÄ±, Ã–ÄŸrenebilirlik, Genelleme, Uyarlanabilirlik, HÄ±z, Analiz, TasarÄ±m KolaylÄ±ÄŸÄ±
<br>

# 2.3.	YSAâ€™nÄ±n DezavantajlarÄ±

## EÄŸitim SÃ¼resi: 
EÄŸitilmek iÃ§in uzun bir zamana ihtiyaÃ§ duyarlar ve bundan dolayÄ± zaman ve para maliyeti yÃ¼ksektir.
## EÄŸitim HatalarÄ±: 
Bir problemin Ã§Ã¶zÃ¼mÃ¼nde Ã§ok uygun bir Ã§Ã¶zÃ¼m bulamayabilirler ya da hata yapabilirler. Bunun sebebi, aÄŸÄ± eÄŸitecek bir fonksiyonun bulunmamasÄ±dÄ±r. BazÄ± durumlarda fonksiyon bulunsa bile yeterli veri bulunamayabilir.
## AÄŸÄ±n DavranÄ±ÅŸlarÄ±nÄ±n AÃ§Ä±klanamamasÄ±: 
Bu, YSA'larÄ±n en Ã¶nemli sorunudur. YSA bir probleme Ã§Ã¶zÃ¼m Ã¼rettiÄŸi zaman, bunun neden ve nasÄ±l olduÄŸuna iliÅŸkin bir ipucu vermez. Bu durum aÄŸa olan gÃ¼veni azaltÄ±cÄ± bir unsurdur.
## DonanÄ±m BaÄŸÄ±mlÄ± OlmasÄ±: 
Yapay sinir aÄŸlarÄ± yapÄ±sÄ± gereÄŸi paralel iÅŸlem gÃ¼cÃ¼ne sahip iÅŸlemcilere ihtiyaÃ§ duymaktadÄ±r. Bu nedenle gerÃ§ekleÅŸtirilmesi donanÄ±ma baÄŸÄ±mlÄ±dÄ±r.
## AÄŸÄ±n EÄŸitim SÃ¼resinin Bilinmemesi: 
AÄŸÄ±n Ã¶rnekler Ã¼zerindeki hatasÄ±nÄ±n belirli bir deÄŸerin altÄ±na indirilmesi eÄŸitimin tamamlandÄ±ÄŸÄ± anlamÄ±na gelmektedir. Bu deÄŸer bize optimum neticeler vermemektedir.
## Problemin AÄŸa GÃ¶sterim ZorluÄŸu: 
YSA'lar nÃ¼merik bilgiler ile Ã§alÄ±ÅŸabilmektedirler. Problemler YSA'lara tanÄ±tÄ±lmadan Ã¶nce nÃ¼merik deÄŸerlere Ã§evrilmek zorundadÄ±rlar. Burada belirlenecek gÃ¶sterim mekanizmasÄ± aÄŸÄ±n performansÄ±nÄ± doÄŸrudan etkileyecektir. Bu da kullanÄ±cÄ±nÄ±n yeteneÄŸine baÄŸlÄ±dÄ±r.


<br>


# 2.3.	YSA YapÄ±sÄ±

Ä°lk yapay sinir aÄŸÄ± modeli 1943 yÄ±lÄ±nda, bir sinir hekimi olan Warren McCulloch ile bir matematikÃ§i olan Walter Pitts tarafÄ±ndan gerÃ§ekleÅŸtirilmiÅŸtir. McCulloch ve Pitts, insan beyninin hesaplama yeteneÄŸinden esinlenerek, elektrik devreleriyle basit bir sinir aÄŸÄ± modellemiÅŸlerdir.

YSA ile basit biyolojik sinir sisteminin Ã§alÄ±ÅŸma ÅŸekli taklit edilir. Biyolojik nÃ¶ron hÃ¼crelerinin ve bu hÃ¼crelerin birbirleri ile arasÄ±nda kurduÄŸu sinaptik baÄŸÄ±n dijital olarak modellenmesidir. Yapay sinir aÄŸlarÄ± yapay sinir hÃ¼crelerinin birbirine baÄŸlanmasÄ±yla oluÅŸan yapÄ±lardÄ±r. 
<br>

![GÃ¶rsel 1](./doc_img/7.jpg "GÃ¶rsel 1")

Yapay sinir aÄŸlarÄ± Ã¼Ã§ ana katmanda incelenir; GiriÅŸ KatmanÄ±, Ara (Gizli) Katmanlar ve Ã‡Ä±kÄ±ÅŸ KatmanÄ±.
<br>

# 2.4.1.	Biyolojik NÃ¶ron HÃ¼cresi Modeli
4 bÃ¶lÃ¼mden oluÅŸurlar :
*dendrit 
*gÃ¶vde
*akson 
*Ã§ekirdek

![GÃ¶rsel 1](./doc_img/8.jpg "GÃ¶rsel 1")


Biyolojik sinir aÄŸlarÄ±nÄ±n sinir hÃ¼creleri gibi YSAâ€™larÄ±n da sinir hÃ¼creleri vardÄ±r. Buna proses elemanÄ± denir. Her bir proses elemanÄ± 5 temel kÄ±sÄ±mdan oluÅŸur.

*GiriÅŸler
*AÄŸÄ±rlÄ±klar
*Toplama Fonksiyonu
*Aktivasyon Fonksiyonu
*Ã‡Ä±kÄ±ÅŸ

![GÃ¶rsel 1](./doc_img/9.png "GÃ¶rsel 1")

GiriÅŸler A ile gÃ¶sterilmektedir. Bu giriÅŸlerin her biri aÄŸÄ±rlÄ±k olan W ile Ã§arpÄ±lÄ±r. Elde edilen bilgi eÅŸik deÄŸeri ile toplanÄ±r ve sonucu oluÅŸturmak iÃ§in Aktivasyon Fonksiyonu ile iÅŸlem yapÄ±lÄ±r. Bu iÅŸlemler sonucunda y Ã§Ä±ktÄ±sÄ± alÄ±nÄ±r. TÃ¼m sinirsel aÄŸlarÄ±n yapÄ±sÄ± bu temele dayanarak oluÅŸturulur. YSAâ€™nÄ±n Ã¶ÄŸrenme yeteneÄŸi aÄŸÄ±rlÄ±klarÄ±n sÃ¼reli olarak gÃ¼ncellenip ayarlanmasÄ± ile doÄŸru orantÄ±lÄ±dÄ±r.

<br>

# 2.5.	YSA Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±
Girdiler nÃ¶ronlara gelen verilerdir. Bu girdilerden gelen veriler biyolojik sinir hÃ¼crelerinde olduÄŸu gibi toplanmak Ã¼zere nÃ¶ron Ã§ekirdeÄŸine gÃ¶nderilir. AÄŸÄ±rlÄ±klar, yapay sinir hÃ¼cresine gelen bilgiler girdiler Ã¼zerinden Ã§ekirdeÄŸe ulaÅŸmadan Ã¶nce geldikleri baÄŸlantÄ±larÄ±n aÄŸÄ±rlÄ±ÄŸÄ±yla Ã§arpÄ±larak Ã§ekirdeÄŸe iletilir. Bu sayede girdilerin Ã¼retilecek Ã§Ä±ktÄ± Ã¼zerindeki etkisi ayarlanabilmektedir. Toplama fonksiyonu bir yapay sinir hÃ¼cresine aÄŸÄ±rlÄ±klarla Ã§arpÄ±larak gelen girdileri toplayarak o hÃ¼crenin net girdisini hesaplayan bir fonksiyondur. Aktivasyon fonksiyonu, Ã¶nceki katmandaki tÃ¼m girdilerin aÄŸÄ±rlÄ±klÄ± toplamÄ±nÄ± alan ve daha sonra bir Ã§Ä±kÄ±ÅŸ deÄŸeri (tipik olarak doÄŸrusal olmayan) Ã¼reten ve bir sonraki katmana geÃ§iren bir fonksiyondur. (Ã¶rneÄŸin, ReLU veya sigmoid ).

![GÃ¶rsel 1](./doc_img/10.png "GÃ¶rsel 1")

Aktivasyon fonksiyonundan Ã§Ä±kan deÄŸer hÃ¼crenin Ã§Ä±ktÄ± deÄŸeri olmaktadÄ±r. Her hÃ¼crenin birden fazla girdisi olmasÄ±na raÄŸmen bir tek Ã§Ä±ktÄ±sÄ± olmaktadÄ±r. Bu Ã§Ä±ktÄ± istenilen sayÄ±da hÃ¼creye baÄŸlanabilir.
<br>

# 2.6.	YSA Modelleri
<br>

# 2.6.1.	YapÄ±larÄ±na GÃ¶re

<br>

# 2.6.1.1.	Ä°leri Beslemeli
<br>

Ä°ÅŸlem giriÅŸlerden Ã§Ä±kÄ±ÅŸlara doÄŸru ilerler. Ã‡Ä±kÄ±ÅŸ deÄŸerleri Ã¶ÄŸreticiden alÄ±nan istenen Ã§Ä±kÄ±ÅŸ deÄŸerleri ile karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r ve bir hata sinyali elde edilerek aÄŸ aÄŸÄ±rlÄ±klarÄ± gÃ¼ncellenir. Ä°leri beslemeli yapay sinir aÄŸlarÄ±nda gecikmeler yoktur. Kendi aralarÄ±nda Tek katmanlÄ± ileri beslemeli aÄŸlar ve Ã‡ok katmanlÄ± ileri beslemeli aÄŸlar olarak ayrÄ±lÄ±rlar.

<br>

# 2.6.1.1.1.	Tek KatmanlÄ± Ä°leri Beslemeli
En basit aÄŸ tipi olup bir Ã§Ä±ktÄ± katmanÄ± ve buna baÄŸlÄ± bir girdi katmanÄ±ndan oluÅŸmaktadÄ±r.

![GÃ¶rsel 1](./doc_img/11.png "GÃ¶rsel 1")

<br>

# 2.6.1.1.2.	Ã‡ok KatmanlÄ± Ä°leri Beslemeli
Girdi katmanÄ± dÄ±ÅŸ ortamlardan aldÄ±ÄŸÄ± bilgileri hiÃ§bir deÄŸiÅŸikliÄŸe uÄŸratmadan orta (gizli) katmandaki hÃ¼crelere iletir. Bilgi, orta ve Ã§Ä±kÄ±ÅŸ katmanÄ±nda iÅŸlenerek aÄŸ Ã§Ä±kÄ±ÅŸÄ± belirlenir.

![GÃ¶rsel 1](./doc_img/12.png "GÃ¶rsel 1")

Ã‡ok katmanlÄ± aÄŸlar tek katmanlÄ± aÄŸlara gÃ¶re daha karmaÅŸÄ±k problemlerin Ã§Ã¶zÃ¼mÃ¼nde kullanÄ±lÄ±rlar. Ã‡ok katmanlÄ± aÄŸlarÄ±n eÄŸitilmesi zordur.

<br>

# 2.6.1.2.	Geri Beslemeli 

Geri beslemeli sinir aÄŸÄ±, ileri beslemeli bir aÄŸÄ±n Ã§Ä±kÄ±ÅŸlarÄ±nÄ±n giriÅŸlere baÄŸlanmasÄ± ile elde edilir. Geri beslemeli sinir aÄŸlarÄ±nda gecikmeler vardÄ±r. Geri beslemeli sinir aÄŸlarÄ±, hÃ¼creler arasÄ± veya katmanlar arasÄ± geri besleme yapÄ±lÄ±ÅŸ ÅŸekline gÃ¶re farklÄ± isimlerle sÃ¶ylenir.

![GÃ¶rsel 1](./doc_img/13.jpg "GÃ¶rsel 1")

<br>

# 2.6.1.2.1.	Tam Geri Beslemeli
Bu aÄŸlar geliÅŸigÃ¼zel ileri ve geri baÄŸlantÄ±larÄ± olan aÄŸlardÄ±r. Bu baÄŸlantÄ±larÄ±n hepsi eÄŸitilebilir.
<br>

# 2.6.1.2.2.	KÄ±smi Geri Beslemeli
Bu aÄŸlarda, aÄŸÄ±n hÃ¼cre elemanlarÄ±na ek olarak iÃ§erik (context) elemanlarÄ± vardÄ±r. Geri besleme sadece iÃ§erik elemanlarÄ± Ã¼zerinde yapÄ±lÄ±r ve bu baÄŸlantÄ±lar eÄŸitilemezler. Ä°Ã§erik elemanlarÄ± ara katman elemanlarÄ±nÄ±n geÃ§miÅŸ durumlarÄ±nÄ± hatÄ±rlamak iÃ§in kullanÄ±lÄ±r.
<br>

# 2.6.2.	Ã–ÄŸrenme AlgoritmalarÄ±na GÃ¶re

<br>

# 2.6.2.1.	DanÄ±ÅŸmanlÄ± (Supervised) Ã–ÄŸrenme
EÄŸitim sÄ±rasÄ±nda sisteme bir girdi ve bir hedef Ã§Ä±ktÄ± vektÃ¶rlerinin Ã§ift olarak verilmesi ve bunlara gÃ¶re sistemdeki aÄŸÄ±rlÄ±k deÄŸerlerinin gÃ¼ncellenmesi ve deÄŸiÅŸtirilmesi yapÄ±lÄ±r. Yapay sinir aÄŸlarÄ±nÄ±n eÄŸitilmesinde kullanÄ±lan hedef Ã§Ä±ktÄ± 1 veya 0 olabileceÄŸi gibi bir Ã¶rÃ¼ntÃ¼ de olabilir. Belli bir bilgi kÃ¼mesine karÅŸÄ±lÄ±k, ilgili Ã§Ä±ktÄ± kÃ¼mesini hatÄ±rlayacak ÅŸekilde eÄŸitilmiÅŸ sistemlere â€œÃ§aÄŸrÄ±ÅŸÄ±mlÄ± bellekâ€ denir. EÄŸer, girdi vektÃ¶rÃ¼ ile Ã§Ä±ktÄ± vektÃ¶rÃ¼ aynÄ± ise buna â€œÃ¶z Ã§aÄŸrÄ±ÅŸÄ±mlÄ± bellekâ€, Ã§Ä±ktÄ± vektÃ¶rÃ¼ farklÄ± ise buna da â€œkarÅŸÄ±t Ã§aÄŸrÄ±ÅŸÄ±mlÄ± bellekâ€ denir.

![GÃ¶rsel 1](./doc_img/14.png "GÃ¶rsel 1")


<br>

# 2.6.2.2.	DanÄ±ÅŸmansÄ±z (Unsupervised) Ã–ÄŸrenme
Bu sistemlerde, bir grup girdi vektÃ¶rÃ¼ sisteme verilir, ancak hedef Ã§Ä±ktÄ±lar belirtilmez.Sistem girdiler iÃ§erisinde birbirine en Ã§ok benzeyenleri gruplar ve her bir grup iÃ§in farklÄ± bir Ã¶rÃ¼ntÃ¼ tanÄ±mlar.Ã–zdÃ¼zenlemeli Ã¶zellik haritalarÄ± bu yÃ¶ntemi kullanarak sÄ±nÄ±flama iÅŸlemini yerine getirir.

![GÃ¶rsel 1](./doc_img/15.jpg "GÃ¶rsel 1")

Grossberg tarafÄ±ndan geliÅŸtirilen ART (Adaptive Resonance Theory) veya Kohonen tarafÄ±ndan geliÅŸtirilen SOM(Self Organizing Map) Ã¶ÄŸrenme kuralÄ± danÄ±ÅŸmansÄ±z Ã¶ÄŸrenmeye Ã¶rnek olarak verilebilir.



<br>

# 2.6.2.3.	Takviyeli (Reinforcement) Ã–ÄŸrenme
Bu Ã¶ÄŸrenme kuralÄ± danÄ±ÅŸmanlÄ± Ã¶ÄŸrenmeye yakÄ±n bir metotdur. Hedef Ã§Ä±ktÄ±yÄ± vermek iÃ§in bir â€œÃ¶ÄŸretmenâ€ yerine, burada yapay sinir aÄŸÄ±na bir Ã§Ä±kÄ±ÅŸ verilmemekte fakat elde edilen Ã§Ä±kÄ±ÅŸÄ±n verilen giriÅŸe karÅŸÄ±lÄ±k iyiliÄŸini deÄŸerlendiren bir kriter kullanÄ±lmaktadÄ±r. Takviyeli Ã¶ÄŸrenmede, aÄŸÄ±n davranÄ±ÅŸlarÄ±nÄ±n uygun olup olmadÄ±ÄŸÄ±nÄ± belirten bir Ã¶zyetenek bilgisine ihtiyaÃ§ duyulur.Bu bilgiye gÃ¶re aÄŸÄ±rlÄ±klar ayarlanÄ±r. GerÃ§ek zamanda Ã¶ÄŸrenme olup, deneme-yanÄ±lma esasÄ±na gÃ¶re sinir aÄŸÄ± eÄŸitilmektedir.

![GÃ¶rsel 1](./doc_img/16.jpg "GÃ¶rsel 1")

Optimizasyon problemlerini Ã§Ã¶zmek iÃ§in Hinton ve Sejnowskiâ€™nin geliÅŸtirdiÄŸi Boltzman KuralÄ± veya Genetik Algoritmalar takviyeli Ã¶ÄŸrenmeye Ã¶rnek olarak verilebilir.

<br>

# 2.6.2.4.	Karma Stratejiler
DanÄ±ÅŸmanlÄ±, danÄ±ÅŸmansÄ±z veya takviyeli Ã¶ÄŸrenme stratejilerinden birkaÃ§Ä±nÄ± birlikte kullanarak geliÅŸtirilen yapÄ±dÄ±r.Radial TabanlÄ± (Radial Basis Network) aÄŸlar ve OlasÄ±lÄ±k TabanlÄ± (Probabilistic Neural Network) aÄŸlar bunlara Ã¶rnek olarak verilebilir.
<br>

# 2.6.3.	Ã–ÄŸrenme ZamanÄ±na GÃ¶re YSA Modelleri

<br>

# 2.6.3.1.	Statik
YSA eÄŸitim verileriyle eÄŸitilir ve aÄŸÄ±n yapÄ±sÄ± kaydedilir. AÄŸ bundan sonra hep aynÄ± yapÄ±yla Ã§alÄ±ÅŸÄ±r. KullanÄ±mÄ± sÄ±rasÄ±nda herhangi bir deÄŸiÅŸikliÄŸe uÄŸramaz
<br>

# 2.6.3.2.	Dinamik
YSA eÄŸitim verileriyle eÄŸitildikten sonra kullanÄ±mÄ± sÄ±rasÄ±nda da kendini dÃ¼zenlemeye devam eder. BÃ¶ylece sÃ¼rekli Ã¶ÄŸrenen bir YSA elde edilebilinir.
<br>


# 2.7.	YSAâ€™nÄ±n Ã‡alÄ±ÅŸmasÄ±

Yapay sinir aÄŸlarÄ±nÄ±n yapÄ± taÅŸÄ± olan yapay sinir hÃ¼creleri kendilerine verilen girdileri her bir girdiye ait aÄŸÄ±rlÄ±ÄŸa gÃ¶re deÄŸerlendirerek Ã§Ä±kÄ±ÅŸÄ±nÄ± Ã¼retir.

![GÃ¶rsel 1](./doc_img/17.png "GÃ¶rsel 1")

YSA giriÅŸ katmanÄ±ndan aldÄ±ÄŸÄ± verileri ara katmanlarda iÅŸleyerek Ã§Ä±kÄ±ÅŸ katmanÄ±na iletir.


![GÃ¶rsel 1](./doc_img/18.png "GÃ¶rsel 1")

<br>


# 2.7.1.	Back Propagation

Yapay sinir aÄŸlarÄ±nda en Ã§ok kullanÄ±lan Ã¶ÄŸrenme algoritmalarÄ±ndan biri geri yayÄ±lmalÄ± (back propagation) Ã¶ÄŸrenme algoritmasÄ±dÄ±r. Bu algoritmada her iterasyon sonucunda Ã§Ä±kÄ±ÅŸ katmanÄ±ndaki hata hesaplanarak bu hata Ã§Ä±kÄ±ÅŸ katmanÄ±ndan giriÅŸ katmanÄ±na doÄŸru bÃ¼tÃ¼n nÃ¶ronlara iletilir ve aÄŸÄ±rlÄ±klar hata payÄ±na gÃ¶re tekrar dÃ¼zenlenir.


![GÃ¶rsel 1](./doc_img/19.jpg "GÃ¶rsel 1")


Geri yayÄ±lma algoritmasÄ±nda Ã§Ä±kÄ±ÅŸ katmanÄ±ndaki nÃ¶rona ait hata payÄ± kendinden Ã¶nceki nÃ¶ronlara aÄŸÄ±rlÄ±klarÄ±yla orantÄ±lÄ± olarak daÄŸÄ±tÄ±lÄ±r. Her nÃ¶ron iÃ§in hata payÄ± bulunduktan sonra bu hata paylarÄ± yardÄ±mÄ±yla yeni aÄŸÄ±rlÄ±klar hesaplanÄ±r.




<br>

# 2.8.	YSA Ã–ÄŸrenme KurallarÄ±

<br>

# 2.8.1.	Ã‡evrimiÃ§i (On-line) Ã–ÄŸrenme KurallarÄ± 

Bu kurallara gÃ¶re Ã¶ÄŸrenen sistemler gerÃ§ek zamanda Ã§alÄ±ÅŸÄ±rken bir taraftan fonksiyonlarÄ±nÄ± yerine getirmekte diÄŸer taraftan ise Ã¶ÄŸrenmeye devam etmektedir. ART ve Kohonen Ã¶ÄŸrenme kuralÄ± bu sÄ±nÄ±fa girmektedir.
<br>

# 2.8.2.	Ã‡evrimdÄ±ÅŸÄ± (Off-line) Ã–ÄŸrenme KurallarÄ±
Bu kurallarÄ± kullanan sistemler eÄŸitildikten sonra gerÃ§ek hayatta kullanÄ±ma alÄ±ndÄ±ÄŸÄ±nda artÄ±k Ã¶ÄŸrenme olmamaktadÄ±r. Sistemin Ã¶ÄŸrenmesi gereken yeni bilgiler sÃ¶z konusu olduÄŸunda sistem kullanÄ±mdan Ã§Ä±karÄ±lmakta ve Ã§evrimdÄ±ÅŸÄ± olarak yeniden eÄŸitilmektedir.
<br>

# 2.9.	YSA'nÄ±n TasarÄ±mÄ±
Bir yapay sinir aÄŸÄ±nÄ±n tasarlanmasÄ±nda Ã¶ncelikle gereksinimler belirlenip buna gÃ¶re uygun bir YSA modeli kullanÄ±lÄ±r. YSA tasarÄ±mÄ±nda aÅŸaÄŸÄ±daki kriterler belirlenmelidir.

*Ã–ÄŸrenme AlgoritmasÄ±
*AÄŸdaki Katman SayÄ±sÄ±
*Her Bir Katmandaki NÃ¶ron SayÄ±sÄ±
*NÃ¶ronlarÄ±n Aktivasyon Fonksiyonu

# 2.10.	Ã–rnek YSA Modelleri

![GÃ¶rsel 1](./doc_img/20.gif "GÃ¶rsel 1")

ÃœÃ§ katmanlÄ±, Ã¼Ã§ giriÅŸli, bir Ã§Ä±kÄ±ÅŸlÄ±, ileri beslemeli, geri yayÄ±lmalÄ± Ã¶ÄŸrenme algoritmasÄ± 
<br>

# 2.11.	Yapay Sinir AÄŸlarÄ±nda En Ã‡ok KullanÄ±lan Modeller

Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±lar
Kohonen AÄŸÄ±
CounterPropogation AÄŸÄ±
AlgÄ±layÄ±cÄ±lar (Perceptronlar)
Hoppfield AÄŸÄ±
Adaptive Rezorans Teorisi Modeli (ART)
Lineer VektÃ¶r Quantization Modeli (LVQ)
Jordan AÄŸÄ±
Elman AÄŸÄ±
Probabilistic AÄŸÄ±
Neocognitron AÄŸÄ±
Boltzman Makinesi
Kendi Kendini Organize Eden Model (SOM)
Radyal Temelli AÄŸ(RBN)

<br>


# 2.12.	Yapay Sinir AÄŸlarÄ±nÄ±n EÄŸitilmesi

YSAâ€™da hÃ¼cre elemanlarÄ±nÄ±n baÄŸlantÄ±larÄ±nÄ±n aÄŸÄ±rlÄ±k deÄŸerlerinin belirlenmesi iÅŸlemine â€œaÄŸÄ±n eÄŸitilmesiâ€ denir.BaÅŸlangÄ±Ã§ta bu aÄŸÄ±rlÄ±k deÄŸerleri rastgele alÄ±nÄ±r.YSAâ€™lar, kendilerine Ã¶rnekler gÃ¶sterildikÃ§e bu aÄŸÄ±rlÄ±k deÄŸerlerini yenileyerek amaca ulaÅŸmaya Ã§alÄ±ÅŸÄ±rlar.
<br>

![GÃ¶rsel 1](./doc_img/21.png "GÃ¶rsel 1")

Amaca ulaÅŸmanÄ±n veya yaklaÅŸmanÄ±n Ã¶lÃ§Ã¼sÃ¼ de yine dÄ±ÅŸarÄ±dan verilen bir deÄŸerdir.EÄŸer yapay sinir aÄŸÄ± verilen giriÅŸ-Ã§Ä±kÄ±ÅŸ Ã§iftleriyle amaca ulaÅŸmÄ±ÅŸ ise aÄŸÄ±rlÄ±k deÄŸerleri saklanÄ±r.AÄŸÄ±rlÄ±klarÄ±n sÃ¼rekli yenilenip istenilen sonuca ulaÅŸÄ±lana kadar geÃ§en zamana â€œÃ¶ÄŸrenmeâ€ denir.AÄŸÄ±rlÄ±k deÄŸerlerinin deÄŸiÅŸmesi belirli kurallara gÃ¶re yÃ¼rÃ¼tÃ¼lmektedir. Bu kurallara â€œÃ¶ÄŸrenme kurallarÄ±â€ denir.


Yapay sinir aÄŸÄ± Ã¶ÄŸrendikten sonra daha Ã¶nce verilmeyen giriÅŸler uygulanarak aÄŸ Ã§Ä±kÄ±ÅŸlarÄ± gÃ¶zlemlenir.Genelde eldeki Ã¶rneklerin %80â€™i aÄŸa verilip aÄŸ eÄŸitilir. Daha sonra geri kalan %20â€™lik kÄ±sÄ±m verilip aÄŸÄ±n davranÄ±ÅŸlarÄ± incelenir ve bu iÅŸleme â€œaÄŸÄ±n test edilmesiâ€ denir. EÄŸitimde kullanÄ±lan Ã¶rnekler setine â€œeÄŸitim setiâ€, test iÃ§in kullanÄ±lan sete ise â€œtest setiâ€ denir.

# 2.13.	YSAâ€™nÄ±n Uygulama AlanlarÄ±


<br>



# 2.13.1.	Ses TanÄ±ma
GiriÅŸleri -yani konuÅŸulan bir kelimenin hesaplanan katsayÄ±larÄ±nÄ±- veririz. Ã‡Ä±kÄ±ÅŸlarÄ±n hesaplanmasÄ± iÃ§in YSAâ€™yÄ± Ã§alÄ±ÅŸtÄ±rÄ±rÄ±z. Daha sonra programÄ±nÄ±zda Ã§Ä±kÄ±ÅŸlarÄ± inceleyerek Ã§Ä±kÄ±ÅŸlarÄ±n gÃ¶sterdiÄŸi koda gÃ¶re sÃ¶ylenen kelimeyi anlamaya Ã§alÄ±ÅŸÄ±rÄ±z. AyrÄ±ca bu yÃ¶ntemle sadece ses deÄŸil benzer yapÄ±daki hemen hemen her ÅŸeyi belli bir doÄŸruluk ile tanÄ±yabilir, bir birinden ayÄ±rabilirsiniz. Bunlara Ã¶rnek olarak matematiksel fonksiyonlar, elle veya makine ile yazÄ±lmÄ±ÅŸ karakterler ya da elektronik devrelerin giriÅŸ ve Ã§Ä±kÄ±ÅŸlarÄ± verilebilir.

<br>



# 2.13.2.	TÄ±p AlanÄ±nda
YSAâ€™nÄ±n kalp krizi teÅŸhisinde kullanÄ±lmasÄ± ve doktorlardan daha iyi sonuÃ§ almasÄ±.

<br>

# 2.13.3.	Ä°laÃ§ GeliÅŸtirme
Milli saÄŸlÄ±k kuruluÅŸlarÄ±ndaki araÅŸtÄ±rmalar AÄ°DS ve Kanseri tedavi etmek amacÄ±yla ilaÃ§ geliÅŸtirme sÃ¼recinde YSAâ€™larÄ± kullanmaktadÄ±rlar. YSAâ€™lar ayrÄ±ca biomolekÃ¼lleride modelleme sÃ¼recinde de kullanÄ±lÄ±r.
<br>

# 2.13.4.	Pazarlama
Departman izleme ve uygun tavsiyeler verme iÃ§in kullanÄ±lÄ±r.
<br>


# 2.13.5.	Finans 
BankacÄ±lÄ±kta, kredi kartÄ± ÅŸirketleri ve faiz kurumlarÄ± net olmayan kararlarla uÄŸraÅŸÄ±r.Bu alanlar Ã¶ÄŸrenme ve istatistiki eÄŸilim gerektirir.
<br>


# 2.13.6.	HaberleÅŸme
Ses dalgalandÄ±rma, Dizayn, YÃ¶netim, YÃ¶nlendirme, Kontrol, AÄŸ gÃ¶zetimi iÅŸlerinde kullanÄ±lÄ±r.

<br>


YukarÄ±da belirttiÄŸimiz alanlar ve daha bir Ã§ok alanda da kullanÄ±lmaktadÄ±r. (Ãœretim, Arazi analizi ve tespiti, Otomasyon ve Kontrol, Savunma sanayi, GÃ¶rÃ¼ntÃ¼ iÅŸleme, Ä°ÅŸaret iÅŸleme, Desen tanÄ±ma, Askeri sistemler, GÃ¼Ã§ sistemleri

# **3.	RNN AlgoritmasÄ±**
RNNâ€™ler genelde bir sonraki adÄ±mÄ± tahmin etmek iÃ§in kullanÄ±lan bir Ã§eÅŸit Derin Ã–ÄŸrenme yapÄ±larÄ±dÄ±r. DiÄŸer derin Ã¶ÄŸrenme yapÄ±larÄ±ndan en bÃ¼yÃ¼k farklarÄ± ise hatÄ±rlamalarÄ±dÄ±r.Bir diÄŸer farklarÄ± ise, diÄŸer sinir aÄŸlarÄ±nda her girdi birbirinden baÄŸÄ±msÄ±z iken RNNâ€™lerde girdiler birbiri ile iliÅŸkilidir.RNNâ€™ler bir sonraki adÄ±mÄ± takip edebilmek iÃ§in girdiler arasÄ±nda iliÅŸki kurarlar ve eÄŸitilirken tÃ¼m iliÅŸkilerini hatÄ±rlarlar.
		
RNNâ€™ler kurmuÅŸ olduklarÄ± iliÅŸkilerin kalÄ±cÄ± olmasÄ± iÃ§in kendi iÃ§lerinde dÃ¶nen dÃ¶ngÃ¼ benzeri bir yapÄ± kullanÄ±rlar.

![GÃ¶rsel 1](./doc_img/22.jpg "GÃ¶rsel 1")
<br>

# 3.1.	RNN AlgoritmasÄ± Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±
GiriÅŸ katmanÄ± 'x', sinir aÄŸÄ±na girdi alÄ±r ve onu iÅŸler ve orta katmana iletir. Orta katman 'h', her biri kendi aktivasyon fonksiyonlarÄ±na, aÄŸÄ±rlÄ±klarÄ±na ve sapmalarÄ±na sahip birden Ã§ok gizli katmandan oluÅŸabilir. FarklÄ± gizli katmanlarÄ±n Ã§eÅŸitli parametrelerinin Ã¶nceki katmandan etkilenmediÄŸi bir sinir aÄŸÄ±nÄ±z varsa, yani: sinir aÄŸÄ±nÄ±n belleÄŸi yoksa, o zaman tekrarlayan bir sinir aÄŸÄ± kullanabilirsiniz. Tekrarlayan Sinir AÄŸÄ±, her gizli katmanÄ±n aynÄ± parametrelere sahip olmasÄ± iÃ§in farklÄ± aktivasyon fonksiyonlarÄ±nÄ± ve aÄŸÄ±rlÄ±klarÄ± ve Ã¶nyargÄ±larÄ± standartlaÅŸtÄ±racaktÄ±r. ArdÄ±ndan, birden Ã§ok gizli katman oluÅŸturmak yerine, bir tane oluÅŸturacak ve gerektiÄŸi kadar dÃ¶ngÃ¼ yapacaktÄ±r.

![GÃ¶rsel 1](./doc_img/23.png "GÃ¶rsel 1")



# 3.2.	RNN AlgoritmasÄ± Tarihi
Yinelemeli sinir aÄŸlarÄ± David Rumelhart'Ä±n 1986 yÄ±lÄ±ndaki Ã§alÄ±ÅŸmasÄ±na dayanÄ±r. 1993 yÄ±lÄ±nda, bir RNN Ã§alÄ±ÅŸmasÄ± 1000'den fazla katman gerektiren bir â€œÃ§ok derin Ã¶ÄŸrenmeâ€ gÃ¶revini baÅŸarmÄ±ÅŸtÄ±r. Long short-term memory (LSTM) aÄŸlarÄ± Hochreiter ve Schmidhuber tarafÄ±ndan 1997 yÄ±lÄ±nda geliÅŸtirilmiÅŸ ve Ã§eÅŸitli uygulama alanlarÄ±nda en iyi performanslarÄ± kaydetmiÅŸtir.

<br>



# 3.3.	RNN AlgoritmasÄ± MatematiÄŸi
h_t: Åu anki h deÄŸeri
h_t-1: Bir Ã¶nceki h deÄŸeri
x_t: Åu anki girdi vektÃ¶rÃ¼

![GÃ¶rsel 1](./doc_img/24.png "GÃ¶rsel 1")

<br>

# 3.4.	RNN AlgoritmasÄ± Ã‡eÅŸitleri
<br>


# 3.4.1.	Tam Yinelemeli
<br>

Tam yinelemeli sinir aÄŸlarÄ±nda tÃ¼m nÃ¶ronlarÄ±n Ã§Ä±ktÄ±sÄ± tÃ¼m nÃ¶ronlarÄ±n girdisine baÄŸlanÄ±r. En genel RNN mimarisi budur, Ã§Ã¼nkÃ¼ diÄŸer tÃ¼m mimariler, buradaki bazÄ± baÄŸlarÄ±n aÄŸÄ±rlÄ±klarÄ± sÄ±fÄ±rlanarak elde edilebilir. RNN'ler iki farklÄ± biÃ§imde gÃ¶sterilir: kapalÄ± biÃ§imde, Ã¶zyineleme baÄŸlantÄ±larÄ± dÃ¼ÄŸÃ¼mlerin kendilerinin bir sonraki adÄ±mdaki durumuna olan baÄŸlantÄ±larÄ±dÄ±r; aÃ§Ä±lmÄ±ÅŸ biÃ§imde, dÃ¼ÄŸÃ¼mlerin her zaman adÄ±mÄ±ndaki durumlarÄ± ayrÄ± ayrÄ± gÃ¶sterilir.

![GÃ¶rsel 1](./doc_img/25.png "GÃ¶rsel 1")


# 3.4.2.	GeÃ§itli Yinelemeli
GeÃ§itli yineleme birimi (gated recurrent unit, GRU) 2014 yÄ±lÄ±nda Ã¶nerilmiÅŸ bir yinelemeli aÄŸ birimidir. Bu birimler, nÃ¶ronlar arasÄ±ndaki geÃ§iÅŸi dÃ¼zenleyen bir takÄ±m Ã¶ÄŸeler barÄ±ndÄ±rÄ±r. LSTM'e benzer ÅŸekilde unutma kapÄ±sÄ± bulunur, ancak GRU yapÄ±larÄ± genellikle daha basittir. Polifonik mÃ¼zik ve konuÅŸma sinyali modelleme gibi iÅŸlerde LSTM'e benzer bir baÅŸarÄ±yla Ã§alÄ±ÅŸÄ±r.

![GÃ¶rsel 1](./doc_img/26.png "GÃ¶rsel 1")


# 3.5.	Vanishing Gradient Problemi
Aktivasyon fonksiyonlarÄ± sayesinde girdimizi belirli bir aralÄ±ÄŸa indirgeyebiliriz. Bu aralÄ±k genelde -1 ve 1 veya 0 ve 1 aralÄ±ÄŸÄ±dÄ±r. Ufak bir alana indirgediÄŸimiz iÃ§in girdimizdeki bÃ¼yÃ¼k bir deÄŸiÅŸim aktivasyon fonksiyonunda o kadar bÃ¼yÃ¼k bir deÄŸiÅŸime yol aÃ§mayabilir.
RNNâ€™de Ã§ok erken aÅŸamalarda dahi bu durum gerÃ§ekleÅŸebilir. 
Bu katmanlar Ã¶ÄŸrenmediÄŸi iÃ§in, RNNâ€™ler daha uzun metinlerde gÃ¶rdÃ¼klerini unutabilir ve bÃ¶ylece kÄ±sa sÃ¼reli bir hafÄ±zaya sahip olurlar.

# 3.6.	Vanishing Gradient Problemi Ã‡Ã¶zÃ¼mÃ¼ LTSM

LSTM yapÄ±sÄ± iÃ§erisindeki kapÄ±lar (gate) neyin hatÄ±rlanacaÄŸÄ±nÄ±, neyin unutulacaÄŸÄ±nÄ± belirler. Yani gelen girdi Ã¶nemsizse unutulur, Ã¶nemliyse bir sonraki aÅŸamaya aktarÄ±lÄ±r. Bunu Gate ve Cell State yardÄ±mÄ±yla yapar.
# 3.6.1.	Forget Gate (Unutma KapÄ±sÄ±)
Hangi bilginin tutulacaÄŸÄ± veya unutulacaÄŸÄ±na karar verir. MantÄ±ÄŸÄ± bir sayÄ± 0 ile Ã§arpÄ±lÄ±rsa ne kadar bÃ¼yÃ¼k olursa olsun sonuÃ§ 0 olur. Burada da unutmak iÃ§in girdinin aÄŸÄ±rlÄ±ÄŸÄ±na 0 verilir.
Bir Ã¶nceki gizli katmandan gelen bilgiler ve gÃ¼ncel bilgiler Sigmoid Fonksiyonundan geÃ§er. 0'a ne kadar yakÄ±nsa o kadar unutulacak, 1'e ne kadar yakÄ±nsa o kadar tutulacaktÄ±r.

# 3.6.2.	Input Gate (Girdi KapÄ±sÄ±)
Cell Stateâ€™i gÃ¼ncellemek iÃ§in kullanÄ±lÄ±r. Ã–ncelikle Forget Gateâ€™de (Unutma KapÄ±sÄ±) olduÄŸu gibi Sigmoid fonksiyonu uygulanÄ±r, hangi bilginin tutulacaÄŸÄ±na karar verilir. Daha sonra aÄŸÄ± dÃ¼zenlemek iÃ§in Tanh fonksiyonu yardÄ±mÄ±yla -1,1 arasÄ±na indirgenir ve Ã§Ä±kan iki sonuÃ§ Ã§arpÄ±lÄ±r.

# 3.6.3.	Cell State

Cell Stateâ€™in hÃ¼cre iÃ§erisindeki en Ã¶nemli gÃ¶revi bilgiyi taÅŸÄ±maktÄ±r. TaÅŸÄ±nmasÄ± gereken verileri alÄ±r ve hÃ¼cre sonuna, oradan da diÄŸer hÃ¼crelere taÅŸÄ±r. Yani aÄŸ Ã¼zerinde veri akÄ±ÅŸÄ±nÄ± Cell State yardÄ±mÄ±yla saÄŸlarÄ±z. Ä°lk olarak Forget Gateâ€™den (Unutma KapÄ±sÄ±) gelen sonuÃ§ ile bir Ã¶nceki katmanÄ±n sonucu Ã§arpÄ±lÄ±r. Daha sonra Input Gateâ€™den (Girdi KapÄ±sÄ±) gelen deÄŸer ile toplanÄ±r.


# 3.6.4.	Output Gate

Bir sonraki katmana gÃ¶nderilecek deÄŸere karar verir. Bu deÄŸer, tahmin iÃ§in kullanÄ±lÄ±r. Ã–ncelikle bir Ã¶nceki deÄŸer ile ÅŸu anki girdi Sigmoid fonksiyonundan geÃ§er. Cell Stateâ€™den gelen deÄŸer Tanh fonksiyonundan geÃ§tikten sonra iki deÄŸer Ã§arpÄ±lÄ±r ve bir sonraki katmana â€œBir Ã¶nceki deÄŸerâ€ olarak gider. Cell State ilerler.


![GÃ¶rsel 1](./doc_img/27.png "GÃ¶rsel 1")

# 3.7.	RNN AlgoritmasÄ± AvantajlarÄ±

Bir Ã¶nceki Ã¶rnek ile iliÅŸki kurar. Bu sayede girdiler unutulmadan ilerlenir. KullanÄ±m alanÄ± Ã§ok geniÅŸtir. (Metin ve ses verileri, sÄ±nÄ±flandÄ±rma problemleri, regresyon problemleri, Ã¼retken (generative) modellerde kullanÄ±lÄ±r.) Girdi bÃ¼yÃ¼klÃ¼ÄŸÃ¼yle model boyutunun artmamasÄ±. Zaman iÃ§inde aÄŸÄ±rlÄ±klarÄ±n paylaÅŸÄ±lmasÄ±

# 3.8.	RNN AlgoritmasÄ± DezavanatajlarÄ±

Uzun girdilerin iÅŸlenmesinin zor olmasÄ±. YavaÅŸ hesaplama yapmasÄ± ,Uzun zaman Ã¶nceki bilgiye eriÅŸme zorluÄŸu, Mevcut durum iÃ§in gelecekteki herhangi bir girdinin dÃ¼ÅŸÃ¼nÃ¼lememesi


# 3.9.	RNN AlgoritmasÄ± KullanÄ±m AlanlarÄ±

# 3.9.1.	Resim YazÄ±sÄ± Ekleme

![GÃ¶rsel 1](./doc_img/28.png "GÃ¶rsel 1")

RNN'ler, mevcut etkinlikleri analiz ederek bir gÃ¶rÃ¼ntÃ¼ye altyazÄ± eklemek iÃ§in kullanÄ±lÄ±r.

# 3.9.2.	Zaman Serisi Tahmini
![GÃ¶rsel 1](./doc_img/29.png "GÃ¶rsel 1")

Belirli bir aydaki hisse senedi fiyatlarÄ±nÄ± tahmin etmek gibi herhangi bir zaman serisi sorunu bir RNN kullanÄ±larak Ã§Ã¶zÃ¼lebilir.

# 3.9.3.	DoÄŸal Dil Ä°ÅŸleme
![GÃ¶rsel 1](./doc_img/30.png "GÃ¶rsel 1")


Metin madenciliÄŸi, Duygu analizi, DoÄŸal Dil Ä°ÅŸleme (NLP) bir RNN kullanÄ±larak gerÃ§ekleÅŸtirilebilir.



# 3.9.4.	Makine Ã‡evirisi
![GÃ¶rsel 1](./doc_img/31.png "GÃ¶rsel 1")

Bir dilde girdi verildiÄŸinde, girdiyi Ã§Ä±ktÄ± olarak farklÄ± dillere Ã§evirmek iÃ§in RNN'ler kullanÄ±labilir.

# **4.	KaynakÃ§a**

>Biswal, A. (2020, April 24). Recurrent Neural Network (RNN) Tutorial: Types and Examples [Updated] | Simplilearn. Simplilearn.com. https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn

>Wikipedia. (n.d.). Wikipedia. https://www.wikipedia.org/

>Biswal, A. (2020, April 24). Recurrent Neural Network (RNN) Tutorial: Types and Examples [Updated] | Simplilearn. Simplilearn.com. https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn
Wikipedia. (n.d.). Wikipedia. https://www.wikipedia.org/
Yapay Sinir AÄŸÄ±(Artificial Neural Network) Nedir? - Veri Bilimi Okulu. (n.d.). Veri Bilimi Okulu. https://www.veribilimiokulu.com/yapay-sinir-agiartificial-neural-network-nedir/

>Biswal, A. (2020, April 24). Recurrent Neural Network (RNN) Tutorial: Types and Examples [Updated] | Simplilearn. Simplilearn.com. https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn
Sinir AÄŸÄ± Nedir? Yapay Zeka ve Makine Ã–ÄŸrenimi KÄ±lavuzu - AWS. (n.d.). Amazon Web Services, Inc. https://aws.amazon.com/tr/what-is/neural-network/
Wikipedia. (n.d.). Wikipedia. https://www.wikipedia.org/
Yapay Sinir AÄŸÄ±(Artificial Neural Network) Nedir? - Veri Bilimi Okulu. (n.d.). Veri Bilimi Okulu. https://www.veribilimiokulu.com/yapay-sinir-agiartificial-neural-network-nedir/


>Yapay Sinir AÄŸÄ±(Artificial Neural Network) Nedir? - Veri Bilimi Okulu. (n.d.). Veri Bilimi Okulu. https://www.veribilimiokulu.com/yapay-sinir-agiartificial-neural-network-nedir/

>What are Recurrent Neural Networks? | IBM. (n.d.-d). IBM - Deutschland | IBM. https://www.ibm.com/topics/recurrent-neural-networks
